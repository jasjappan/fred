## Fred the robot arm

Table of Contents
-----------------
  * [Obstacle avoidance with reinforcement learning](#Obstacle-avoidance-with-reinforcement-learning)
  * [Algorithm details](#Algorithm-details)
  * [deploying in the real world](#deploying-in-the-real-world)

## Obstacle avoidance with reinforcement learning
Using deep reinforcement learning the robot can reach the goal whilst avoiding obstacles, it even outperforms a classical method (gradient descent). 

Gradient descent (left) vs trained agent (right):

<p float="left">
  <img src="media/gd.gif" width="335" />
  <img src="media/RL.gif" width="335" />
</p>

With gradient descent the robot simply gets stuck. The trained agent manages to complete the task and can even be deployed on a real robot:

![Alt text](media/fred.gif)

## Algorithm details
The goal is to get the robot to move from point A to point B without bumping into obstacles. 
A classical method is to use workspace potential fields and gradient descent, which is just a fancy name for 
pulling the robot towards the target and pushing it away from obstacles. However, the robot can get stuck when the net force is zero. 
The input for the gradient descent algorithm are 2 types of vectors: attractive vectors pointing towards the goal position and repulsive vectors pointing away from obstacles. 
These vectors originate from control points that have to be chosen manually:
![Alt text](media/gradientDescentVectors.PNG)

If the net force on the robot is zero it gets stuck. I cannot "see" the obstacle except when it gets close to it.


This is where the idea of using reinforcement learning came from, we can give the neural network the same inputs, but we can also make it aware of the obstacles it's about to face. 
In theory, it could then plan its movement ahead and avoid the obstacle without getting stuck. The idea is to take the obstacles and turn them into an occupancy grid which is then 
fed into the neural network as an image:
![Alt text](media/grid_conversion.png)

The attractive vectors, repulsive vectors and the occupancy grid should be all the information needed to reach the goal, and it turns out it is.
The specific algorithm used here is the soft actor critic method ([Soft Actor Critic__ Haarnoja et al., 2018](https://arxiv.org/abs/1812.05905)).

The agent controls the robot by updating its target position and orientation (pose). Giving the agent direct control over the angels of the robot would mean it has to figure out inverse kinematics as well, but since I have the algorithm for inverse kinematics there is no need for this.
This does mean however that the movement generated by the agent looks a little jagged, so the output of the agent is then used to generate a nice smooth curve using a B-spline:

Raw movement (left) vs smoothed movement (right):

<p float="left">
  <img src="media/rl_raw.gif" width="335" />
  <img src="media/rl.gif" width="335" />
</p>

Here is the agent solving all the training scenarios:

<p float="left">
  <img src="media/solve_1.gif" width="335" />
  <img src="media/solve_2.gif" width="335" />
</p>


## deploying in the real world


## Reinforcement learning training with docker
If you want to save yourself the pain of installing cuda, there is always docker. 

Training instructions:

build with: docker build -t thomas137/sac -f SaCDockerFile .

Run with:
docker run -it --gpus all --rm -v /home/thomas/PycharmProjects/fred/src:/tf/src   thomas137/sac

once in docker:
python src/reinforcementlearning/softActorCritic/soft_actor_critic.py --root_dir=test