Index: src/reinforcementlearning/softActorCritic/soft_actor_critic.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport inspect\nimport os\nimport time\n\nimport gin\nimport tensorflow as tf\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.metrics import tf_py_metric\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\nfrom tf_agents.system import system_multiprocessing as multiprocessing\nimport multiprocessing as mp\n\nfrom src.reinforcementlearning.softActorCritic.behavioral_cloning import fill_replay_buffer_with_gradient_descent\nfrom src.reinforcementlearning.softActorCritic.sac_utils import create_agent, compute_metrics, save_checkpoints, \\\n    make_and_initialze_checkpointers, print_time_progression, initialize_and_restore_train_checkpointer, create_envs\n\nflags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n                    'Root directory for writing logs/summaries/checkpoints.')\nflags.DEFINE_string('behavioral_cloning_checkpoint_dir', None,\n                    'Directory in the root dir where the results for the behavioral cloning are saved')\n\nflags.DEFINE_multi_string('gin_file', None, 'Path to the trainer config files.')\nflags.DEFINE_multi_string('gin_param', None, 'Gin binding to pass through.')\n\nFLAGS = flags.FLAGS\n\n\ndef train_eval(checkpoint_dir,\n               checkpoint_dir_behavioral_cloning=None,\n               total_train_steps=2000000,\n               # Params for collect,\n               initial_collect_steps=10000,\n               collect_steps_per_iteration=150,\n               replay_buffer_capacity=1000000,\n               # Params for target update,\n               # Params for train,\n               train_steps_per_iteration=150,\n               batch_size=256,\n               use_tf_functions=True,\n               # Params for eval,\n               num_eval_episodes=1,\n               eval_interval=2500,\n               # Params for summaries and logging,\n               train_checkpoint_interval=5000,\n               policy_checkpoint_interval=5000,\n               rb_checkpoint_interval=50000,\n               log_interval=5000,\n               summary_interval=1000,\n               summaries_flush_secs=10,\n               robot_env_no_obstacles=True,\n               num_parallel_environments=16):\n    current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n\n    root_dir = os.path.expanduser(current_dir + '/checkpoints/' + checkpoint_dir)\n    train_dir = os.path.join(root_dir, 'train')\n    eval_dir = os.path.join(root_dir, 'eval')\n\n    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n        train_dir, flush_millis=summaries_flush_secs * 1000)\n    train_summary_writer.set_as_default()\n\n    eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n        eval_dir, flush_millis=summaries_flush_secs * 1000)\n    eval_metrics = [\n        tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n        tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n    ]\n\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n    with tf.compat.v2.summary.record_if(\n            lambda: tf.math.equal(global_step % summary_interval, 0)):\n\n        tf_env, eval_tf_env = create_envs(robot_env_no_obstacles, num_parallel_environments)\n\n        tf_agent = create_agent(tf_env, global_step, robot_env_no_obstacles)\n\n        if checkpoint_dir_behavioral_cloning is not None:\n            restore_agent_from_behavioral_cloning(current_dir, checkpoint_dir_behavioral_cloning, tf_agent, global_step)\n\n        environment_steps_metric = tf_metrics.EnvironmentSteps()\n        step_metrics = [\n            tf_metrics.NumberOfEpisodes(),\n            environment_steps_metric,\n        ]\n\n        # Make the replay buffer.\n        replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n            data_spec=tf_agent.collect_data_spec,\n            batch_size=tf_env.batch_size,\n            max_length=replay_buffer_capacity)\n        replay_observer = [replay_buffer.add_batch]\n\n        train_metrics = step_metrics + [\n            tf_metrics.NumberOfEpisodes(),\n            tf_metrics.EnvironmentSteps(),\n            tf_py_metric.TFPyMetric(py_metrics.AverageReturnMetric(batch_size=tf_env.batch_size)),\n            tf_py_metric.TFPyMetric(py_metrics.AverageEpisodeLengthMetric(batch_size=tf_env.batch_size)),\n        ]\n\n        eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n        initial_collect_policy = random_tf_policy.RandomTFPolicy(\n            tf_env.time_step_spec(), tf_env.action_spec())\n        collect_policy = tf_agent.collect_policy\n\n        train_checkpointer, policy_checkpointer, rb_checkpointer = make_and_initialze_checkpointers(train_dir,\n                                                                                                    tf_agent,\n                                                                                                    global_step,\n                                                                                                    eval_policy,\n                                                                                                    replay_buffer,\n                                                                                                    train_metrics)\n\n        print(\"replay buffer size: {}\".format(replay_buffer.num_frames().numpy()))\n\n        initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n            tf_env,\n            initial_collect_policy,\n            observers=replay_observer,\n            num_steps=initial_collect_steps)\n\n        collect_driver = dynamic_step_driver.DynamicStepDriver(\n            tf_env,\n            collect_policy,\n            observers=replay_observer + train_metrics,\n            num_steps=collect_steps_per_iteration)\n\n        if use_tf_functions:\n            initial_collect_driver.run = common.function(initial_collect_driver.run)\n            collect_driver.run = common.function(collect_driver.run)\n            tf_agent.train = common.function(tf_agent.train)\n\n        if global_step.numpy() == 0:\n            # Collect initial replay data.\n            logging.info(\n                'Initializing replay buffer by collecting experience for %d steps with '\n                'a random policy.', initial_collect_steps)\n            # initial_collect_driver.run()\n            with tf.device('/CPU:0'):\n                fill_replay_buffer_with_gradient_descent(tf_env, initial_collect_steps, robot_env_no_obstacles, replay_buffer)\n        else:\n            logging.info(\"skipping initial collect because we already have data\")\n\n        # For debugging\n        # tf.config.experimental_run_functions_eagerly(False)\n\n        compute_metrics(eval_metrics, eval_tf_env, eval_policy, num_eval_episodes, global_step, eval_summary_writer)\n\n        time_step = None\n        policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n        timed_at_step = global_step.numpy()\n        time_acc = 0\n\n        dataset = replay_buffer.as_dataset(\n            sample_batch_size=batch_size,\n            num_steps=2).unbatch().batch(batch_size).prefetch(5)\n        iterator = iter(dataset)\n\n        def train_step():\n            experience, _ = next(iterator)\n            return tf_agent.train(experience=experience)\n\n        if use_tf_functions:\n            train_step = common.function(train_step)\n\n        time_before_training = time.time()\n\n        steps_taken_in_prev_round = global_step.numpy()  # From a previous training round\n        while global_step.numpy() < total_train_steps:\n            global_steps_taken = global_step.numpy()\n\n            start_time = time.time()\n\n            for _ in range(train_steps_per_iteration):\n                 train_loss = train_step()\n\n            time_step, policy_state = collect_driver.run(\n                time_step=time_step,\n                policy_state=policy_state,\n            )\n\n            time_acc += time.time() - start_time\n\n            if global_steps_taken % log_interval == 0:\n                logging.info('step = %d, loss = %f', global_steps_taken,\n                             train_loss.loss)\n                steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n                logging.info('%.3f steps/sec', steps_per_sec)\n                tf.compat.v2.summary.scalar(\n                    name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n                timed_at_step = global_step.numpy()\n                time_acc = 0\n\n                print(\"current step: {}\".format(global_steps_taken))\n                print_time_progression(time_before_training, global_steps_taken - steps_taken_in_prev_round,\n                                       total_train_steps - steps_taken_in_prev_round)\n\n            for train_metric in train_metrics:\n                train_metric.tf_summaries(\n                    train_step=global_step, step_metrics=train_metrics[:2])\n\n            if global_steps_taken % eval_interval == 0:\n                compute_metrics(eval_metrics, eval_tf_env, eval_policy, num_eval_episodes, global_step,\n                                eval_summary_writer)\n\n            save_checkpoints(global_steps_taken, train_checkpoint_interval, policy_checkpoint_interval,\n                             rb_checkpoint_interval, train_checkpointer, policy_checkpointer, rb_checkpointer)\n\n        time_after_trianing = time.time()\n\n        elapsed_time = time_after_trianing - time_before_training\n        print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n\n\ndef restore_agent_from_behavioral_cloning(current_dir, checkpoint_dir_behavioral_cloning, tf_agent, global_step):\n    print(\"restoring agent from the behavioral cloning run\")\n    root_dir_behavioral_cloning = os.path.expanduser(current_dir + '/checkpoints/' + checkpoint_dir_behavioral_cloning)\n    train_dir_behavioral_cloning = os.path.join(root_dir_behavioral_cloning, 'train')\n    initialize_and_restore_train_checkpointer(train_dir_behavioral_cloning, tf_agent, global_step)\n\n\ndef main(_):\n    tf.compat.v1.enable_v2_behavior()\n    logging.set_verbosity(logging.INFO)\n    gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n\n    print(\"GPU Available: \", tf.test.is_gpu_available())\n\n    print(\"eager is on: {}\".format(tf.executing_eagerly()))\n\n    print(\"Cores available: {}\".format(mp.cpu_count()))\n\n    # if not tf.bc.is_gpu_available():\n    #     print(\"no point in training without a gpu, go watch the grass grow instead\")\n    #     sys.exit()\n    # tf.config.experimental_run_functions_eagerly(True)\n    train_eval(FLAGS.root_dir, FLAGS.behavioral_cloning_checkpoint_dir)\n\n\n# PYTHONUNBUFFERED=1;LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64\nif __name__ == '__main__':\n    flags.mark_flag_as_required('root_dir')\n    multiprocessing.handle_main(functools.partial(app.run, main))\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/reinforcementlearning/softActorCritic/soft_actor_critic.py	(revision 1ab3497ef69a380b919b30f03cdebd83e47a45fe)
+++ src/reinforcementlearning/softActorCritic/soft_actor_critic.py	(date 1606070101094)
@@ -60,7 +60,7 @@
                log_interval=5000,
                summary_interval=1000,
                summaries_flush_secs=10,
-               robot_env_no_obstacles=True,
+               robot_env_no_obstacles=False,
                num_parallel_environments=16):
     current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
 
